{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffq2HEUTOsm6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apache Spark DataFrames Assignment Project**\n",
        "\n",
        "Problem statement##\n",
        "As a Data professional, you need to perform an analysis by answering questions about some stock market data on Safaricom from the years 2012-2017\n",
        "\n",
        "Required actions\n",
        "Data Importation and Exploration:\n",
        "\n",
        "● Start a spark session and load the stock file while inferring the data types.\n",
        "\n",
        "● Determine the column names\n",
        "\n",
        "● Make observations about the schema.\n",
        "\n",
        "● Show the first 5 rows\n",
        "\n",
        "● Use the describe method to learn about the data frame\n",
        "\n",
        "Data Preparation\n",
        "\n",
        "● Format all the data to 2 decimal places i.e. format_number()\n",
        "\n",
        "● Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
        "\n",
        "Data Analysis\n",
        "\n",
        "● What day had the Peak High in Price?\n",
        "\n",
        "● What is the mean of the Close column?\n",
        "\n",
        "● What is the max and min of the Volume column?\n",
        "\n",
        "● How many days was the Close lower than 60 dollars?\n",
        "\n",
        "● What percentage of the time was the High greater than 80 dollars?\n",
        "\n",
        "● What is the Pearson correlation between High and Volume?\n",
        "\n",
        "● What is the max High per year?\n",
        "\n",
        "● What is the average Close for each Calendar Month?\n",
        "\n",
        "Data Source :Dataset URL (CSV File): https://bit.ly/3pmchka"
      ],
      "metadata": {
        "id": "QLHc-aB3Pasf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pyspark instalation\n",
        "\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJILmQetP8zX",
        "outputId": "2726db2e-d18f-4fdc-95b6-69d8ea8c34a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=e03c359db2670bb62c81dba3f3569ef1f092c62f9e9567339530cfca2ddb0b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run the spark session\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "spark = SparkSession.builder.appName(\"stock_market_data\").getOrCreate()\n",
        "sql_context = SQLContext(spark)\n"
      ],
      "metadata": {
        "id": "DZyZuOO8Rh9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828d0e1b-4b5b-428f-afdc-0dd97f87022f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a Spark session and read a file with inferred data types\n",
        "stock_saf_df = spark.read.csv(\"saf_stock.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "M6ta5KZqS6kt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names of the DataFrame\n",
        "print(stock_saf_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm90L_ybUJiU",
        "outputId": "8e33c564-ddd6-48ec-823f-cbe680e4e1a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the schema of the DataFrame\n",
        "stock_saf_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwUffJV1Ud8j",
        "outputId": "f05ff15d-4389-40d2-e16a-09839e1681d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows of a DataFrame\n",
        "stock_saf_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0SGpPW9VbAQ",
        "outputId": "8a1e3337-3907-4915-b13e-84f2d032d794"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the DataFrame\n",
        "stock_saf_df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrwVS-HMWLbO",
        "outputId": "7dde491e-cc78-4a80-9ff6-14c02f4b34e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning and Transformation**"
      ],
      "metadata": {
        "id": "VdpDQHE7W6V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format numeric columns to 2 decimal places\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "numeric_cols = [col[0] for col in stock_saf_df.dtypes if col[1] in ['double', 'int']]\n",
        "df = stock_saf_df.select(\"Date\", *[format_number(col, 2).alias(col) for col in numeric_cols])\n",
        "\n",
        "# Show the first 5 rows of the formatted DataFrame\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i-Bqa5_XAMb",
        "outputId": "d1dc8a3c-4efd-423b-be4e-ceb2a8b55e3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "|      Date| Open| High|  Low|Close|       Volume|Adj Close|\n",
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12,668,800.00|    52.62|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9,593,300.00|    52.08|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12,768,200.00|    51.83|\n",
            "|2012-01-06|59.42|59.45|58.87|59.00| 8,069,400.00|    51.46|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6,679,300.00|    51.62|\n",
            "+----------+-----+-----+-----+-----+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Remove commas from Volume column\n",
        "stock_saf_df = stock_saf_df.withColumn(\"Volume\", regexp_replace(col(\"Volume\"), \",\", \"\"))\n",
        "\n",
        "# Calculate HV ratio and create new column\n",
        "stock_saf_df = stock_saf_df.withColumn(\"HV Ratio\", stock_saf_df.High/ stock_saf_df.Volume)\n",
        "\n",
        "# Show the first 5 rows of the resulting dataframe\n",
        "stock_saf_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DRkHcTvZdR7",
        "outputId": "01c802cb-2dee-4168-f495-1dd240a89966"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|            HV Ratio|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|7.367338463826307E-6|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|8.915604778943901E-6|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analysis: Steps and Overview.**"
      ],
      "metadata": {
        "id": "-0ESqKsCZ570"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SQL context\n",
        "sql_ctx = SQLContext(spark)\n",
        "\n",
        "# Create temporary view of DataFrame\n",
        "stock_saf_df.createOrReplaceTempView(\"stock_data\")\n",
        "\n",
        "# Get list of tables in SQL context\n",
        "print(\"Tables in SQLContext:\", sql_ctx.tableNames())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw6pHyzXZ87l",
        "outputId": "cdfdf82f-6685-477c-e41f-b5f13db8dc8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables in SQLContext: ['stock_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query DataFrame to get date and high price of day with highest high price\n",
        "peak_high_price = stock_saf_df.select(\"Date\", \"High\").orderBy(\"High\", ascending=False).limit(1)\n",
        "\n",
        "# Display the resulting data\n",
        "peak_high_price.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge9jhN0hapcV",
        "outputId": "65b986b5-9a1f-4aea-b424-4f3e23c8feae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|      Date|     High|\n",
            "+----------+---------+\n",
            "|2015-01-13|90.970001|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean of the close column\n",
        "close_mean = stock_saf_df.selectExpr(\"mean(Close)\").collect()[0][0]\n",
        "\n",
        "# Display the resulting mean\n",
        "print(\"Mean of close column:\", close_mean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ZFOIAqbCQB",
        "outputId": "9a23b589-74fb-479f-c062-44df8551b86c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of close column: 72.38844998012726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get minimum and maximum values of Volume column\n",
        "volume_stats = stock_saf_df.selectExpr(\"min(Volume)\", \"max(Volume)\").first()\n",
        "\n",
        "# Display the resulting statistics\n",
        "print(\"Minimum Volume:\", volume_stats[0])\n",
        "print(\"Maximum Volume:\", volume_stats[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZZAYxE3bLIL",
        "outputId": "7b40e8f2-bd20-4059-b8af-c9a39fe19eea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Volume: 10010500\n",
            "Maximum Volume: 9994400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of days with close price < 60\n",
        "days_60_count = stock_saf_df.filter(\"Close < 60\").count()\n",
        "\n",
        "# Display the resulting count\n",
        "print(\"Number of days with close price < 60:\", days_60_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHdeUrujbetC",
        "outputId": "3a2e93c8-edb6-4dcf-ce73-df74bdf382d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of days with close price < 60: 81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentage of time where High is >80$\n",
        "high_count = stock_saf_df.filter(\"High > 80\").count()\n",
        "total_count = stock_saf_df.count()\n",
        "percentage = (high_count / total_count) * 100\n",
        "\n",
        "# Display the resulting percentage\n",
        "print(\"% time where High is >80$: {:.2f}%\".format(percentage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7LLHO44cCRh",
        "outputId": "8f5a38aa-58b2-46ee-87a9-87bd92794e08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% time where High is >80$: 9.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import corr function from PySpark\n",
        "from pyspark.sql.functions import corr\n",
        "\n",
        "# Calculate Pearson correlation coefficient between High and Volume\n",
        "correlation = stock_saf_df.select(corr(\"High\", \"Volume\")).collect()[0][0]\n",
        "\n",
        "# Display the resulting correlation coefficient\n",
        "print(\"Pearson correlation between High and Volume is:\", correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enhj6FP8cKVn",
        "outputId": "2ce1c100-fccb-4889-b137-aa7b1408282d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation between High and Volume is: -0.3384326061737161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the functions for grouping and aggregating data\n",
        "from pyspark.sql.functions import year, max\n",
        "\n",
        "# Group the stock_saf_df DataFrame by year and calculate the maximum High for each year\n",
        "high_max = stock_saf_df.groupBy(year(\"Date\").alias(\"year\")).agg(max(\"High\").alias(\"max_high\")).orderBy(\"max_high\", ascending=False)\n",
        "\n",
        "# Show the result\n",
        "high_max.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4NDlFQ3dzSr",
        "outputId": "78cb31e3-4372-48f0-9ad8-1ef874a2242e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year| max_high|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2014|88.089996|\n",
            "|2013|81.370003|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "p8RylZJwewpA"
      }
    }
  ]
}